# Автоматическая сегментация японского текста: сравнение словарей

Проект представляет собой расширение более раннего исследования, для которого типы составных единиц были заранее составлены на основе ряда научных публикаций на тему сложных единиц в японском языке, а списки составных единиц - при помощи случайного выбора с использованием корпуса BCCWJ ("Сбалансированный корпус письменных текстов японского языка"). В ходе выполнения данного проекта некоторые списки пополнялись ввиду необходимости проверить появляющиеся гипотезы. В тетради представлены итоговые списки составных единиц, включая те, контексты с которыми найдены не были. В дальнейшем планирую еще расширить исследование за счет включения большего числа типов составных единиц и расширения списка имеющихся.

Цель исследования - анализ сегментации составных единиц японского языка в составе контекстов в связи с используемым словарем:
единицы-канго длины два и три символа с «префиксами»;
единицы-канго длины два и три символа с «постфиксами»;
единицы-канго длины два и три символа с сочинительной связью между элементами (типа «отец и мать» – «родители»);
глаголы-ваго, имеющие более длинные и более краткие варианты написания;
существительные-ваго, имеющие более длинные и более краткие варианты написания.

Использовался морфологический ананлизатор mecab-python3 (оболочка токенизатора MeCab версии 0.996 на языке Python) и три словаря:
UniDic - разработан Государственным институтом японского языка и лингвистики, имеет несколько вариантов: для письменных текстов, для устных текстов	и для текстов различных эпох (в проекте использован первый);
IPAdic - разработан много лет назад Институтом науки и технологий Нара, но его продолжают использовать многие токенизаторы;
JumanDic - разработан Университетом Киото для токенизатора Juman (сейчас версия Juman++).


Этапы работы.

1. Составление корпуса контекстов: парсинг выдачи по каждой из составных единиц в корпусе BCCWJ на сайте корпусов NINJAL с помощью Selenium (с сохранением «сырых» контекстов в csv-файлы)
2. Обработка полученных контекстов: сегментация контекстов MeCab’ом с использованием разных словарей и первичный анализ полученной сегментации (добавление информации, разделена ли сложная единица, на какие единицы, ожидаема ли полученная сегментация и есть ли соединение элементов единицы с соседями по контексту), сохранение обработанных контекстов в новые csv-файлы с разделением по словарям.
3. Вывод и анализ полученных результатов: подсчет статистики по каждой составной единице на основе данных о сегментации, агрегация таблиц по типам составных единиц, сохранение полной статистики в отдельные файлы, вывод результатов с краткой информацией в виде таблиц и графиков.

Тетрадки с кодом и подробным описанием каждого этапа, а также полученные на этапе файлы сохранены в соответствующих папках.
