# Автоматическая сегментация составных японских единиц: сравнение словарей

Проект представляет собой расширение более раннего исследования, для которого типы составных единиц были заранее составлены на основе ряда научных публикаций на тему сложных единиц в японском языке, а списки составных единиц - при помощи случайного выбора с использованием корпуса BCCWJ ("Сбалансированный корпус письменных текстов японского языка"). В ходе выполнения данного проекта некоторые списки пополнялись ввиду необходимости проверить появляющиеся гипотезы. В тетради представлены итоговые списки составных единиц, включая те, контексты с которыми найдены не были. В дальнейшем планирую еще расширить исследование за счет включения большего числа типов составных единиц и расширения списка имеющихся.

Цель исследования - анализ сегментации составных единиц японского языка в составе контекстов в связи с используемым словарем:
единицы-канго длины два и три символа с «префиксами»;
единицы-канго длины два и три символа с «постфиксами»;
единицы-канго длины два и три символа с сочинительной связью между элементами (типа «отец и мать» – «родители»);
глаголы-ваго, имеющие более длинные и более краткие варианты написания;
существительные-ваго, имеющие более длинные и более краткие варианты написания.

Использовался морфологический ананлизатор mecab-python3 (оболочка токенизатора MeCab версии 0.996 на языке Python) и три словаря:
UniDic - разработан Государственным институтом японского языка и лингвистики, имеет несколько вариантов: для письменных текстов, для устных текстов	и для текстов различных эпох (в проекте использован первый);
IPAdic - разработан много лет назад Институтом науки и технологий Нара, но его продолжают использовать многие токенизаторы;
JumanDic - разработан Университетом Киото для токенизатора Juman (сейчас версия Juman++).


Этапы работы.

1. Составление корпуса контекстов: парсинг выдачи по каждой из составных единиц в корпусе BCCWJ на сайте корпусов NINJAL с помощью Selenium (с сохранением «сырых» контекстов в csv-файлы). Такой вариант сбора данных был выбран исходя из необходимости найти контексты с конкретными составными единицами в как можно большем количестве, а сбор контекстов посредством корпуса японских текстов как раз позволял лучше контролировать процесс отбора данных. Код для создания корпуса контекстов вместе с подробной информацией представлен в двух тетрадях, имеющих в названии "corpus_building" (в одной из них - для VCS, во второй - для Colab). Из обеих тетрадей по понятным причинам удалены мои персональные данные (имя пользователя и пароль для входа на сайт корпусов NINJAL). Полученные в результате парсинга файлы находятся в папке "raw_contexts".
2. Обработка полученных контекстов: сегментация контекстов MeCab’ом с использованием разных словарей и первичный анализ полученной сегментации (добавление информации, разделена ли сложная единица, на какие единицы, ожидаема ли полученная сегментация и есть ли соединение элементов единицы с соседями по контексту), сохранение обработанных контекстов в новые csv-файлы с разделением по словарям. Код для обработки "сырых" контекстов хранится в тетради под названием "contexts_segmentation". Полученные в результате анализа файлы хранятся в папке "segmentation_analysis" (во всех папках, кроме "statistics").
3. Вывод и анализ полученных результатов: подсчет статистики по каждой составной единице на основе данных о сегментации, агрегация таблиц по типам составных единиц, сохранение полной статистики в отдельные файлы, вывод результатов с краткой информацией в виде таблиц и графиков. Код и краткую статистику можно посмотреть в тетради под названием "analysis_and_visualization", файлы с полной статистикой по составным единицам хранятся в папке "statistics" в "segmentation_analysis".


Результаты.

Сравнение словарей показало, что разница в результатах сегментации при их использовании есть, хотя часто не слишком большая. В целом словарь unidic показывает большую последовательность в сегментации, чем другие словари. На втором месте - ipadic, jumandic ведет себя несколько более непоследовательно в ряде случаев, иначе, чем другие словари, ведет себя с глаголами, а также склонен присоединять некоторые форманты к единицам текста. Более подробно разницу между словарями можно посмотреть в тетради "analysis_and_visualization", а также в файлах в папке "statistics" (находится в "segmentation_analysis").

В дальнейшем планируется дополнительное расширение исследования за счет включения еще нескольких групп составных единиц, увеличения числа контексов с единицами в имеющихся группах, а также подключения тэггера для анализа выбросов в данных.
